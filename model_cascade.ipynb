{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c730f9b8-3dbe-4ba9-9329-b583e8756c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_experimental.pal_chain import PALChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Note : This program has OPENAI_API_KEY set as an environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bc56e-846f-4c0e-9124-2f735fc41d9f",
   "metadata": {},
   "source": [
    "### Let's set up the template for Tree of Thought. We'll use this for tasks that involve planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bede54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "Here is a question {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors \n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"solutions\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"deepen_thought_process\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution. Finally reword the response as possible steps to take to achieve the goal\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")\n",
    "\n",
    "tot_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d819b-b31e-4e93-ae51-ef7d4331af68",
   "metadata": {},
   "source": [
    "### Set up the LLMs, embedding model and PAL chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_llm = \"gpt-3.5-turbo-instruct\"\n",
    "strong_llm= \"gpt-4\"\n",
    "\n",
    "# Load the pre-trained sentence embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', cache_folder='/mnt/artifacts/model_cache/') # change this location to where you want to store the embedding model\n",
    "\n",
    "pal_llm_temperature = 0.7\n",
    "llm = OpenAI(temperature=pal_llm_temperature, model=weak_llm)\n",
    "\n",
    "pal_chain = PALChain.from_math_prompt(llm=llm, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5eca2e-9394-41ee-97f9-97444d36bfc2",
   "metadata": {},
   "source": [
    "### Setup the CoT chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fe36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. If you do not know the answer or are not confident about the answer reply that you don't know the answer. Do not hallucinate an answer. Just return the number as an answer\"\"\"\n",
    "\n",
    "cot_llm_temperature = 0.7\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "cot_llm_chain = LLMChain(llm=OpenAI(temperature=cot_llm_temperature,\n",
    "                                    model=weak_llm),\n",
    "                         prompt=prompt)\n",
    "cot_strong_llm_chain = LLMChain(llm=ChatOpenAI(model=strong_llm), \n",
    "                                prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6decaa9e-521e-4713-99e8-302bd6ab3443",
   "metadata": {},
   "source": [
    "### Helper function to return the answer that agrees most with the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b66852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_answer(samples):\n",
    "    \"\"\"\n",
    "    Identifies the most similar answer among a list of samples based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - samples (list of dicts): A list where each element is a dict containing an 'text' key.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The most similar answer and the matrix of similarity scores between all answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract texts from samples\n",
    "    answers = [sample['text'] for sample in samples]\n",
    "    \n",
    "    # Encode the answers into vectors and calculate similarity scores\n",
    "    answer_embeddings = embedding_model.encode(answers, convert_to_tensor=True)\n",
    "    similarity_scores = util.pytorch_cos_sim(answer_embeddings, answer_embeddings).numpy()\n",
    "\n",
    "    # Determine the index of the most similar answer\n",
    "    most_similar_idx = np.argmax(np.mean(similarity_scores, axis=1))\n",
    "    most_similar_answer = answers[most_similar_idx]\n",
    "\n",
    "    print(f\"The answer that agrees the most with the majority is: {most_similar_answer}\")\n",
    "    return most_similar_answer, similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8ddd5-a5c3-47dd-8263-cbb71635d2ef",
   "metadata": {},
   "source": [
    "### Implementation of the decision by vote where multiple answers are sampled and the answer that agrees the most with the sampled answer is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0dd1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def decision_by_vote(question: str, n_samples: int = 3, mode: str = 'mot'):\n",
    "    \"\"\"\n",
    "    Decides on an answer by collecting samples based using the specified thought representation.\n",
    "\n",
    "    This function generates sample answers based on the specified mode and then determines the most consistent answer along with its similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The question to be answered by the models.\n",
    "    - n_samples: The number of samples to generate for voting. Default is 3.\n",
    "    - mode: The mode of decision making. Can be 'mot' (use mixture of thought), 'cot' (chain of thought),\n",
    "      'pal' (program assisted learning), or 'tot' (tree of thought). The mode is case insensitive.\n",
    "\n",
    "    Returns:\n",
    "    - llm_answer: The answer selected as the most consistent across generated samples.\n",
    "    - s_score: The s score for the most consistent answer, indicating agreement level among samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate input questions based on mode\n",
    "    mode = mode.lower()\n",
    "    input_key = 'input' if mode == 'tot' else 'question'\n",
    "    input_questions = [{input_key: question}] * n_samples\n",
    "\n",
    "    # Initialize variables\n",
    "    llm_answer = None\n",
    "    similarity_matrix = None\n",
    "    samples = []\n",
    "\n",
    "    # Function to process samples and remove newlines\n",
    "    def process_samples(raw_samples, key):\n",
    "        return [{'text': sample[key]} for sample in raw_samples]\n",
    "\n",
    "    # Fetch and process samples based on mode\n",
    "    if mode in [\"mot\", \"cot\"]:\n",
    "        cot_samples = await cot_llm_chain.aapply(input_questions)\n",
    "        samples.extend(process_samples(cot_samples, 'text'))\n",
    "\n",
    "    if mode in [\"mot\", \"pal\"]:\n",
    "        pal_samples = pal_chain.batch(input_questions)  # Assuming apply can be awaited, if pal_chain supports async\n",
    "        samples.extend(process_samples(pal_samples, 'result'))\n",
    "\n",
    "    if mode == 'tot':\n",
    "        tot_samples = tot_chain.batch(input_questions)  # Assuming apply can be awaited, if tot_chain supports async\n",
    "        samples.extend(process_samples(tot_samples, 'ranked_solutions'))\n",
    "\n",
    "    if samples:\n",
    "        llm_answer, similarity_matrix = get_final_answer(samples)\n",
    "\n",
    "    # Calculate the s-score for the most consistent answer\n",
    "    s_score = np.mean(similarity_matrix[:, np.argmax(similarity_matrix.sum(axis=0))]) if similarity_matrix is not None else 0\n",
    "\n",
    "    return llm_answer, s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfec282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to be used for the different thought representations\n",
    "\n",
    "question  = \"\"\"\n",
    "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
    "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43d638-baca-4b4e-a72e-14c41cee367a",
   "metadata": {},
   "source": [
    "# Get the answers from the vote based method for the different thought representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 3798\n",
      "\tPrompt Tokens: 3432\n",
      "\tCompletion Tokens: 366\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.00588\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the PAL samples.\n",
    "PAL is useful when the question needs reasoning like inferring transitive and associative properties\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "with get_openai_callback() as pal_cb:\n",
    "    pal_results = asyncio.run(decision_by_vote(question=question, mode='pal'))\n",
    "    print(pal_cb)\n",
    "# pal_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a946d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: .\n",
      "\n",
      "\n",
      "First, let's create a list of temperatures in Fahrenheit to store our converted values:\n",
      "Fahrenheit = []\n",
      "\n",
      "Next, let's loop through each temperature in the list of Celsius temperatures and convert them to Fahrenheit using the formula:\n",
      "for temp in Celsius:\n",
      "    fahrenheit = temp * 9/5 + 32\n",
      "    # add the converted temperature to the Fahrenheit list\n",
      "    Fahrenheit.append(fahrenheit)\n",
      "\n",
      "Finally, let's print out the new list of Fahrenheit temperatures to see the converted values:\n",
      "print(Fahrenheit)\n",
      "\n",
      "The output should be: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 905\n",
      "\tPrompt Tokens: 306\n",
      "\tCompletion Tokens: 599\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.001657\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the CoT samples\n",
    "Good for simple reasoning questions like this one about green shirts\n",
    "\"\"\"\n",
    "with get_openai_callback() as cot_cb:\n",
    "    cot_results = asyncio.run(decision_by_vote(question=question, mode='cot'))\n",
    "    print(cot_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b642c-9c2c-4719-88b0-c6550c5c4f9a",
   "metadata": {},
   "source": [
    "#### Note : This will resample from CoT and PAL and will not reuse the samples from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5b5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 4308\n",
      "\tPrompt Tokens: 3738\n",
      "\tCompletion Tokens: 570\n",
      "Successful Requests: 6\n",
      "Total Cost (USD): $0.0067469999999999995\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the MoT samples\n",
    "Useful when we want to be really confident about the answer\n",
    "\"\"\"\n",
    "with get_openai_callback() as mot_cb:\n",
    "    mot_results = asyncio.run(decision_by_vote(question=question, mode='mot'))\n",
    "    print(mot_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bab2d2-fd2f-4bf8-b463-8bdd9f460d01",
   "metadata": {},
   "source": [
    "### Decide whether to accept/reject and call the stronger LLM to get an answer for the vote based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ea1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from vote using gpt-3.5-turbo-instruct: .\n",
      "\n",
      "\n",
      "First, let's create a list of temperatures in Fahrenheit to store our converted values:\n",
      "Fahrenheit = []\n",
      "\n",
      "Next, let's loop through each temperature in the list of Celsius temperatures and convert them to Fahrenheit using the formula:\n",
      "for temp in Celsius:\n",
      "    fahrenheit = temp * 9/5 + 32\n",
      "    # add the converted temperature to the Fahrenheit list\n",
      "    Fahrenheit.append(fahrenheit)\n",
      "\n",
      "Finally, let's print out the new list of Fahrenheit temperatures to see the converted values:\n",
      "print(Fahrenheit)\n",
      "\n",
      "The output should be: [71.6, 64.4, 77.0, 86.0, 75.2]\n"
     ]
    }
   ],
   "source": [
    "# accept the answer from the vote based method if the s_score is greater than a threshold\n",
    "vote_threshold = 0.65\n",
    "\n",
    "# for the green shirts question CoT should give a good answer, for more complex queries PAL/MoT are better\n",
    "answer, s_score = cot_results\n",
    "\n",
    "if s_score >= vote_threshold:\n",
    "    print(f\"Answer from vote using {weak_llm}: {answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")\n",
    "    \n",
    "# uncomment below to get the answer from gpt-4 (sample size=1) and to print the comparison later on\n",
    "# with get_openai_callback() as gpt4_cb:\n",
    "#     result = cot_strong_llm_chain.invoke(question)\n",
    "#     print(gpt4_cb)\n",
    "# llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "# print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d300b-850b-4ce1-b93f-20b5c2ed28e2",
   "metadata": {},
   "source": [
    "### Decide whether to accept/reject and call the stronger LLM to get an answer for the decision based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52bdec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 136\n",
      "\tPrompt Tokens: 109\n",
      "\tCompletion Tokens: 27\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00489\n",
      "Answer from gpt-4: The temperatures in Fahrenheit would be [716, 644, 77, 86, 752]\n"
     ]
    }
   ],
   "source": [
    "# Get the CoT answer and the PAL answer; check if they match\n",
    "pal_answer, _ = pal_results\n",
    "cot_answer, _ = cot_results\n",
    "\n",
    "# Verification threshold\n",
    "verification_threshold = 0.65\n",
    "\n",
    "# Extract the last number in the cot text\n",
    "last_digit = re.findall(r'(\\d+)(?![\\d\\S])', cot_answer)\n",
    "cot_answer = last_digit[-1] if last_digit else cot_answer\n",
    "# get the embeddings for the pal and cot_answers\n",
    "cot_embedding = embedding_model.encode(cot_answer, convert_to_tensor=True)\n",
    "pal_embedding = embedding_model.encode(pal_answer, convert_to_tensor=True)\n",
    "\n",
    "# compare the similarity between the embeddings\n",
    "similarity_score = util.pytorch_cos_sim(cot_embedding, pal_embedding).numpy()\n",
    "\n",
    "# accept answer if similarity score is greater than a threshold\n",
    "if similarity_score >= verification_threshold:\n",
    "    print(f\"Answer from verification using {weak_llm} : {cot_answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee651797-537f-4a0d-a7c2-a5702690fead",
   "metadata": {},
   "source": [
    "### Let's try the Tree of Thought for a more complex and subjective question using the vote based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6576cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: Ranking of solutions in order of promise:\n",
      "\n",
      "1. Building underground habitats on Mars\n",
      "2. Sending robotic missions to Mars\n",
      "3. Terraforming Mars\n",
      "\n",
      "Justification for ranking:\n",
      "- Building underground habitats on Mars offers a more feasible and sustainable solution for supporting human life on the planet, as it addresses the challenges of resource scarcity and environmental risks.\n",
      "- Sending robotic missions to Mars provides valuable data and insights for future human missions, but may not directly address the immediate need for habitable conditions on the planet.\n",
      "- Terraforming Mars is a long-term and highly complex solution that may face ethical, technological, and sustainability challenges.\n",
      "\n",
      "Steps to achieve the goal:\n",
      "1. Conduct feasibility studies and research on building underground habitats on Mars.\n",
      "2. Collaborate with experts in space habitat design and construction to develop prototype habitats.\n",
      "3. Test and refine construction methods in extreme environments on Earth.\n",
      "4. Establish partnerships with relevant stakeholders for funding and support.\n",
      "5. Implement gradual experimentation and continuous maintenance to ensure the sustainability of underground habitats on Mars.\n",
      "\n",
      " gpt3.5 token usage and cost \n",
      "\n",
      "Tokens Used: 6751\n",
      "\tPrompt Tokens: 3316\n",
      "\tCompletion Tokens: 3435\n",
      "Successful Requests: 12\n",
      "Total Cost (USD): $0.011843999999999999\n",
      "\n",
      "\n",
      " gpt-4 token usage and cost \n",
      "\n",
      "\n",
      "Tokens Used: 296\n",
      "\tPrompt Tokens: 24\n",
      "\tCompletion Tokens: 272\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.01704\n"
     ]
    }
   ],
   "source": [
    "# Lets try ToT on a planning type question using the voting mechanism and compare the output with gpt-4\n",
    "# Running ToT takes time\n",
    "\n",
    "question = \"How to colonize Mars?\"\n",
    "\n",
    "with get_openai_callback() as tot_cb:\n",
    "    tot_answer, tot_s_score = asyncio.run(decision_by_vote(question=question, mode='tot'))\n",
    "    print(\"\\n gpt3.5 token usage and cost \\n\")\n",
    "    print(tot_cb)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Do not hallucinate an answer\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "gpt4_cot_llm_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-4\"), prompt=prompt)\n",
    "with get_openai_callback() as gpt4_tot_cb:\n",
    "    gpt4_answer = gpt4_cot_llm_chain.invoke(question)\n",
    "    print(\"\\n\\n gpt-4 token usage and cost \\n\\n\")\n",
    "    print(gpt4_tot_cb)\n",
    "\n",
    "tot_embedding = embedding_model.encode(tot_answer, convert_to_tensor=True)\n",
    "gpt4_embedding = embedding_model.encode(gpt4_answer['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a59952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between gpt-4 answer and gpt3.5 tot answer : 0.7499172687530518\n",
      "ToT 's' score : 0.8435128331184387\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the answers from the vote based ToT and GPT-4 match\n",
    "print(f\"Similarity between gpt-4 answer and gpt3.5 tot answer : {util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0]}\")\n",
    "print(f\"ToT 's' score : {tot_s_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d520aa0-6e7e-42ec-8cd2-77bc7782d592",
   "metadata": {},
   "source": [
    "### Let's compare the cost and the results for the ToT vote based method and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbbb93ff-943b-4a95-ba75-2c6e2ffdd7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to colonize Mars?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_aa2b5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_aa2b5_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_aa2b5_level0_col1\" class=\"col_heading level0 col1\" >ToT(# samples = 3)</th>\n",
       "      <th id=\"T_aa2b5_level0_col2\" class=\"col_heading level0 col2\" >GPT-4 (# samples = 1)</th>\n",
       "      <th id=\"T_aa2b5_level0_col3\" class=\"col_heading level0 col3\" >Answer similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_aa2b5_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_aa2b5_row0_col1\" class=\"data row0 col1\" >0.011840</td>\n",
       "      <td id=\"T_aa2b5_row0_col2\" class=\"data row0 col2\" >0.017040</td>\n",
       "      <td id=\"T_aa2b5_row0_col3\" class=\"data row0 col3\" > </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_aa2b5_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_aa2b5_row1_col1\" class=\"data row1 col1\" >Ranking of solutions in order of promise:\n",
       "\n",
       "1. Building underground habitats on Mars\n",
       "2. Sending robotic missions to Mars\n",
       "3. Terraforming Mars\n",
       "\n",
       "Justification for ranking:\n",
       "- Building underground habitats on Mars offers a more feasible and sustainable solution for supporting human life on the planet, as it addresses the challenges of resource scarcity and environmental risks.\n",
       "- Sending robotic missions to Mars provides valuable data and insights for future human missions, but may not directly address the immediate need for habitable conditions on the planet.\n",
       "- Terraforming Mars is a long-term and highly complex solution that may face ethical, technological, and sustainability challenges.\n",
       "\n",
       "Steps to achieve the goal:\n",
       "1. Conduct feasibility studies and research on building underground habitats on Mars.\n",
       "2. Collaborate with experts in space habitat design and construction to develop prototype habitats.\n",
       "3. Test and refine construction methods in extreme environments on Earth.\n",
       "4. Establish partnerships with relevant stakeholders for funding and support.\n",
       "5. Implement gradual experimentation and continuous maintenance to ensure the sustainability of underground habitats on Mars.</td>\n",
       "      <td id=\"T_aa2b5_row1_col2\" class=\"data row1 col2\" >Colonizing Mars will be a complex process involving several steps. \n",
       "\n",
       "1. Space Travel: We need to develop the technology for long duration space flights, including advanced propulsion systems to reduce travel time, and life support systems to sustain astronauts.\n",
       "\n",
       "2. Mars Habitat: We need to create sustainable habitats that can provide life-supporting conditions. These habitats will need to be able to recycle air and water, and produce food. \n",
       "\n",
       "3. Radiation Protection: Mars has a much thinner atmosphere than Earth, which means it doesn't shield the surface as well from radiation. We will need to develop technology to shield habitats and astronauts from harmful radiation.\n",
       "\n",
       "4. Resource Utilization: We need to develop technology to utilize Mars' resources to minimize reliance on supplies from Earth. This includes extracting water from the ground, mining for materials to build with, and potentially even farming.\n",
       "\n",
       "5. Human Factors: We need to address human factors like physical and mental health during long duration space flights and living on Mars.\n",
       "\n",
       "6. Legal and Ethical Considerations: Lastly, we need to address legal and ethical considerations regarding colonization, such as who has the right to colonize and exploit resources, and how to prevent contamination of Mars with Earth life.\n",
       "\n",
       "Please note that this is a simplified overview and the actual process will be much more complex and time-consuming, requiring collaboration from various countries and scientific fields.</td>\n",
       "      <td id=\"T_aa2b5_row1_col3\" class=\"data row1 col3\" >0.749917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4cfc1dc2e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'ToT(# samples = 3)': [round(tot_cb.total_cost, 5), tot_answer],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_tot_cb.total_cost, 5), gpt4_answer['text']],\n",
    "    'Answer similarity':[\" \",util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0] ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"How to colonize Mars?\")\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4ad27-b119-4e35-a72b-80abc45c60d3",
   "metadata": {},
   "source": [
    "### Lets compare the costs and answers for the different thought representations and GPT-4. \n",
    "#### In most cases the cascade/chain to follow would be CoT -> PAL -> MoT -> GPT4. Once PAL is run, MoT can be modified to reuse the PAL and CoT samples can be reused and therefore not incur any additional LLM costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d313eef1-60ac-49e3-a435-39e6701928e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
      "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c98c9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c98c9_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_c98c9_level0_col1\" class=\"col_heading level0 col1\" >CoT(# samples = 3)</th>\n",
       "      <th id=\"T_c98c9_level0_col2\" class=\"col_heading level0 col2\" >PAL (# samples = 3)</th>\n",
       "      <th id=\"T_c98c9_level0_col3\" class=\"col_heading level0 col3\" >MoT (# samples = 6)</th>\n",
       "      <th id=\"T_c98c9_level0_col4\" class=\"col_heading level0 col4\" >GPT-4 (# samples = 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c98c9_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_c98c9_row0_col1\" class=\"data row0 col1\" >0.001660</td>\n",
       "      <td id=\"T_c98c9_row0_col2\" class=\"data row0 col2\" >0.005880</td>\n",
       "      <td id=\"T_c98c9_row0_col3\" class=\"data row0 col3\" >0.006750</td>\n",
       "      <td id=\"T_c98c9_row0_col4\" class=\"data row0 col4\" >0.012870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c98c9_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_c98c9_row1_col1\" class=\"data row1 col1\" >32</td>\n",
       "      <td id=\"T_c98c9_row1_col2\" class=\"data row1 col2\" >[71.6, 64.4, 77.0, 86.0, 75.2]</td>\n",
       "      <td id=\"T_c98c9_row1_col3\" class=\"data row1 col3\" >[71.6, 64.4, 77.0, 86.0, 75.2]</td>\n",
       "      <td id=\"T_c98c9_row1_col4\" class=\"data row1 col4\" >To convert the temperatures from Celsius to Fahrenheit, we use the formula F = C * 9/5 + 32. \n",
       "\n",
       "For each temperature in the list:\n",
       "\n",
       "22°C = 22 * 9/5 + 32 = 71.6°F\n",
       "18°C = 18 * 9/5 + 32 = 64.4°F\n",
       "25°C = 25 * 9/5 + 32 = 77°F\n",
       "30°C = 30 * 9/5 + 32 = 86°F\n",
       "24°C = 24 * 9/5 + 32 = 75.2°F\n",
       "\n",
       "So, the converted list of temperatures in Fahrenheit is [71.6, 64.4, 77, 86, 75.2].</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4d044d2d00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'CoT(# samples = 3)': [round(cot_cb.total_cost, 5), cot_answer],\n",
    "    'PAL (# samples = 3)': [round(pal_cb.total_cost, 5), pal_answer],\n",
    "    'MoT (# samples = 6)': [round(mot_cb.total_cost, 5), mot_results[0]],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_cb.total_cost, 5), result['text']],\n",
    "}\n",
    "\n",
    "question  = \"\"\"\n",
    "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
    "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \"\"\"\n",
    "\n",
    "print(question)\n",
    "df = pd.DataFrame(data)\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80a3a9e7-9e50-4181-9c72-f268ab2ed323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to colonize Mars?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4b966\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_4b966_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_4b966_level0_col1\" class=\"col_heading level0 col1\" >ToT(# samples = 3)</th>\n",
       "      <th id=\"T_4b966_level0_col2\" class=\"col_heading level0 col2\" >GPT-4 (# samples = 1)</th>\n",
       "      <th id=\"T_4b966_level0_col3\" class=\"col_heading level0 col3\" >Answer similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_4b966_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_4b966_row0_col1\" class=\"data row0 col1\" >0.011840</td>\n",
       "      <td id=\"T_4b966_row0_col2\" class=\"data row0 col2\" >0.017040</td>\n",
       "      <td id=\"T_4b966_row0_col3\" class=\"data row0 col3\" > </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4b966_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_4b966_row1_col1\" class=\"data row1 col1\" >Ranking of solutions in order of promise:\n",
       "\n",
       "1. Building underground habitats on Mars\n",
       "2. Sending robotic missions to Mars\n",
       "3. Terraforming Mars\n",
       "\n",
       "Justification for ranking:\n",
       "- Building underground habitats on Mars offers a more feasible and sustainable solution for supporting human life on the planet, as it addresses the challenges of resource scarcity and environmental risks.\n",
       "- Sending robotic missions to Mars provides valuable data and insights for future human missions, but may not directly address the immediate need for habitable conditions on the planet.\n",
       "- Terraforming Mars is a long-term and highly complex solution that may face ethical, technological, and sustainability challenges.\n",
       "\n",
       "Steps to achieve the goal:\n",
       "1. Conduct feasibility studies and research on building underground habitats on Mars.\n",
       "2. Collaborate with experts in space habitat design and construction to develop prototype habitats.\n",
       "3. Test and refine construction methods in extreme environments on Earth.\n",
       "4. Establish partnerships with relevant stakeholders for funding and support.\n",
       "5. Implement gradual experimentation and continuous maintenance to ensure the sustainability of underground habitats on Mars.</td>\n",
       "      <td id=\"T_4b966_row1_col2\" class=\"data row1 col2\" >Colonizing Mars will be a complex process involving several steps. \n",
       "\n",
       "1. Space Travel: We need to develop the technology for long duration space flights, including advanced propulsion systems to reduce travel time, and life support systems to sustain astronauts.\n",
       "\n",
       "2. Mars Habitat: We need to create sustainable habitats that can provide life-supporting conditions. These habitats will need to be able to recycle air and water, and produce food. \n",
       "\n",
       "3. Radiation Protection: Mars has a much thinner atmosphere than Earth, which means it doesn't shield the surface as well from radiation. We will need to develop technology to shield habitats and astronauts from harmful radiation.\n",
       "\n",
       "4. Resource Utilization: We need to develop technology to utilize Mars' resources to minimize reliance on supplies from Earth. This includes extracting water from the ground, mining for materials to build with, and potentially even farming.\n",
       "\n",
       "5. Human Factors: We need to address human factors like physical and mental health during long duration space flights and living on Mars.\n",
       "\n",
       "6. Legal and Ethical Considerations: Lastly, we need to address legal and ethical considerations regarding colonization, such as who has the right to colonize and exploit resources, and how to prevent contamination of Mars with Earth life.\n",
       "\n",
       "Please note that this is a simplified overview and the actual process will be much more complex and time-consuming, requiring collaboration from various countries and scientific fields.</td>\n",
       "      <td id=\"T_4b966_row1_col3\" class=\"data row1 col3\" >0.749917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4d05b8fbb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'ToT(# samples = 3)': [round(tot_cb.total_cost, 5), tot_answer],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_tot_cb.total_cost, 5), gpt4_answer['text']],\n",
    "    'Answer similarity':[\" \",util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0] ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"How to colonize Mars?\")\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d27fd3-3bd9-4c8c-b809-4f6fcae0645e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
