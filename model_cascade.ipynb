{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c730f9b8-3dbe-4ba9-9329-b583e8756c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_experimental.pal_chain import PALChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Note : This program has OPENAI_API_KEY set as an environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bc56e-846f-4c0e-9124-2f735fc41d9f",
   "metadata": {},
   "source": [
    "### Let's set up the template for Tree of Thought. We'll use this for tasks that involve planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bede54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "Here is a question {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors \n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"solutions\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"deepen_thought_process\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution. Finally reword the response as possible steps to take to achieve the goal\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")\n",
    "\n",
    "tot_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d819b-b31e-4e93-ae51-ef7d4331af68",
   "metadata": {},
   "source": [
    "### Set up the LLMs, embedding model and PAL chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_llm = \"gpt-3.5-turbo-instruct\"\n",
    "strong_llm= \"gpt-4\"\n",
    "\n",
    "# Load the pre-trained sentence embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', cache_folder='/mnt/artifacts/model_cache/') # change this location to where you want to store the embedding model\n",
    "\n",
    "pal_llm_temperature = 0.7\n",
    "llm = OpenAI(temperature=pal_llm_temperature, model=weak_llm)\n",
    "\n",
    "pal_chain = PALChain.from_math_prompt(llm=llm, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5eca2e-9394-41ee-97f9-97444d36bfc2",
   "metadata": {},
   "source": [
    "### Setup the CoT chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fe36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. If you do not know the answer or are not confident about the answer reply that you don't know the answer. Do not hallucinate an answer. Just return the number as an answer\"\"\"\n",
    "\n",
    "cot_llm_temperature = 0.7\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "cot_llm_chain = LLMChain(llm=OpenAI(temperature=cot_llm_temperature,\n",
    "                                    model=weak_llm),\n",
    "                         prompt=prompt)\n",
    "cot_strong_llm_chain = LLMChain(llm=ChatOpenAI(model=strong_llm), \n",
    "                                prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6decaa9e-521e-4713-99e8-302bd6ab3443",
   "metadata": {},
   "source": [
    "### Helper function to return the answer that agrees most with the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b66852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_answer(samples):\n",
    "    \"\"\"\n",
    "    Identifies the most similar answer among a list of samples based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - samples (list of dicts): A list where each element is a dict containing an 'text' key.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The most similar answer and the matrix of similarity scores between all answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract texts from samples\n",
    "    answers = [sample['text'] for sample in samples]\n",
    "    \n",
    "    # Encode the answers into vectors and calculate similarity scores\n",
    "    answer_embeddings = embedding_model.encode(answers, convert_to_tensor=True)\n",
    "    similarity_scores = util.pytorch_cos_sim(answer_embeddings, answer_embeddings).numpy()\n",
    "\n",
    "    # Determine the index of the most similar answer\n",
    "    most_similar_idx = np.argmax(np.mean(similarity_scores, axis=1))\n",
    "    most_similar_answer = answers[most_similar_idx]\n",
    "\n",
    "    print(f\"The answer that agrees the most with the majority is: {most_similar_answer}\")\n",
    "    return most_similar_answer, similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8ddd5-a5c3-47dd-8263-cbb71635d2ef",
   "metadata": {},
   "source": [
    "### Implementation of the decision by vote where multiple answers are sampled and the answer that agrees the most with the sampled answer is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0dd1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def decision_by_vote(question: str, n_samples: int = 3, mode: str = 'mot'):\n",
    "    \"\"\"\n",
    "    Decides on an answer by collecting samples based using the specified thought representation.\n",
    "\n",
    "    This function generates sample answers based on the specified mode and then determines the most consistent answer along with its similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The question to be answered by the models.\n",
    "    - n_samples: The number of samples to generate for voting. Default is 3.\n",
    "    - mode: The mode of decision making. Can be 'mot' (use mixture of thought), 'cot' (chain of thought),\n",
    "      'pal' (program assisted learning), or 'tot' (tree of thought). The mode is case insensitive.\n",
    "\n",
    "    Returns:\n",
    "    - llm_answer: The answer selected as the most consistent across generated samples.\n",
    "    - s_score: The s score for the most consistent answer, indicating agreement level among samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate input questions based on mode\n",
    "    mode = mode.lower()\n",
    "    input_key = 'input' if mode == 'tot' else 'question'\n",
    "    input_questions = [{input_key: question}] * n_samples\n",
    "\n",
    "    # Initialize variables\n",
    "    llm_answer = None\n",
    "    similarity_matrix = None\n",
    "    samples = []\n",
    "\n",
    "    # Function to process samples and remove newlines\n",
    "    def process_samples(raw_samples, key):\n",
    "        return [{'text': sample[key]} for sample in raw_samples]\n",
    "\n",
    "    # Fetch and process samples based on mode\n",
    "    if mode in [\"mot\", \"cot\"]:\n",
    "        cot_samples = await cot_llm_chain.aapply(input_questions)\n",
    "        samples.extend(process_samples(cot_samples, 'text'))\n",
    "\n",
    "    if mode in [\"mot\", \"pal\"]:\n",
    "        pal_samples = pal_chain.batch(input_questions)  # Assuming apply can be awaited, if pal_chain supports async\n",
    "        samples.extend(process_samples(pal_samples, 'result'))\n",
    "\n",
    "    if mode == 'tot':\n",
    "        tot_samples = tot_chain.batch(input_questions)  # Assuming apply can be awaited, if tot_chain supports async\n",
    "        samples.extend(process_samples(tot_samples, 'ranked_solutions'))\n",
    "\n",
    "    if samples:\n",
    "        llm_answer, similarity_matrix = get_final_answer(samples)\n",
    "\n",
    "    # Calculate the s-score for the most consistent answer\n",
    "    s_score = np.mean(similarity_matrix[:, np.argmax(similarity_matrix.sum(axis=0))]) if similarity_matrix is not None else 0\n",
    "\n",
    "    return llm_answer, s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfec282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to be used for the different thought representations\n",
    "\n",
    "question  = \"\"\"\n",
    "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
    "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43d638-baca-4b4e-a72e-14c41cee367a",
   "metadata": {},
   "source": [
    "# Get the answers from the vote based method for the different thought representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 3798\n",
      "\tPrompt Tokens: 3432\n",
      "\tCompletion Tokens: 366\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.00588\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the PAL samples.\n",
    "PAL is useful when the question needs reasoning like inferring transitive and associative properties\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "with get_openai_callback() as pal_cb:\n",
    "    pal_results = asyncio.run(decision_by_vote(question=question, mode='pal'))\n",
    "    print(pal_cb)\n",
    "# pal_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a946d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: .\n",
      "\n",
      "\n",
      "First, let's create a list of temperatures in Fahrenheit to store our converted values:\n",
      "Fahrenheit = []\n",
      "\n",
      "Next, let's loop through each temperature in the list of Celsius temperatures and convert them to Fahrenheit using the formula:\n",
      "for temp in Celsius:\n",
      "    fahrenheit = temp * 9/5 + 32\n",
      "    # add the converted temperature to the Fahrenheit list\n",
      "    Fahrenheit.append(fahrenheit)\n",
      "\n",
      "Finally, let's print out the new list of Fahrenheit temperatures to see the converted values:\n",
      "print(Fahrenheit)\n",
      "\n",
      "The output should be: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 905\n",
      "\tPrompt Tokens: 306\n",
      "\tCompletion Tokens: 599\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.001657\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the CoT samples\n",
    "Good for simple reasoning questions like this one about green shirts\n",
    "\"\"\"\n",
    "with get_openai_callback() as cot_cb:\n",
    "    cot_results = asyncio.run(decision_by_vote(question=question, mode='cot'))\n",
    "    print(cot_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b642c-9c2c-4719-88b0-c6550c5c4f9a",
   "metadata": {},
   "source": [
    "#### Note : This will resample from CoT and PAL and will not reuse the samples from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5b5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 4308\n",
      "\tPrompt Tokens: 3738\n",
      "\tCompletion Tokens: 570\n",
      "Successful Requests: 6\n",
      "Total Cost (USD): $0.0067469999999999995\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the MoT samples\n",
    "Useful when we want to be really confident about the answer\n",
    "\"\"\"\n",
    "with get_openai_callback() as mot_cb:\n",
    "    mot_results = asyncio.run(decision_by_vote(question=question, mode='mot'))\n",
    "    print(mot_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bab2d2-fd2f-4bf8-b463-8bdd9f460d01",
   "metadata": {},
   "source": [
    "### Decide whether to accept/reject and call the stronger LLM to get an answer for the vote based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ea1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from vote using gpt-3.5-turbo-instruct: .\n",
      "\n",
      "\n",
      "First, let's create a list of temperatures in Fahrenheit to store our converted values:\n",
      "Fahrenheit = []\n",
      "\n",
      "Next, let's loop through each temperature in the list of Celsius temperatures and convert them to Fahrenheit using the formula:\n",
      "for temp in Celsius:\n",
      "    fahrenheit = temp * 9/5 + 32\n",
      "    # add the converted temperature to the Fahrenheit list\n",
      "    Fahrenheit.append(fahrenheit)\n",
      "\n",
      "Finally, let's print out the new list of Fahrenheit temperatures to see the converted values:\n",
      "print(Fahrenheit)\n",
      "\n",
      "The output should be: [71.6, 64.4, 77.0, 86.0, 75.2]\n"
     ]
    }
   ],
   "source": [
    "# accept the answer from the vote based method if the s_score is greater than a threshold\n",
    "vote_threshold = 0.65\n",
    "\n",
    "# for the green shirts question CoT should give a good answer, for more complex queries PAL/MoT are better\n",
    "answer, s_score = cot_results\n",
    "\n",
    "if s_score >= vote_threshold:\n",
    "    print(f\"Answer from vote using {weak_llm}: {answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")\n",
    "    \n",
    "# uncomment below to get the answer from gpt-4 (sample size=1) and to print the comparison later on\n",
    "# with get_openai_callback() as gpt4_cb:\n",
    "#     result = cot_strong_llm_chain.invoke(question)\n",
    "#     print(gpt4_cb)\n",
    "# llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "# print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d300b-850b-4ce1-b93f-20b5c2ed28e2",
   "metadata": {},
   "source": [
    "### Decide whether to accept/reject and call the stronger LLM to get an answer for the decision based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52bdec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 136\n",
      "\tPrompt Tokens: 109\n",
      "\tCompletion Tokens: 27\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00489\n",
      "Answer from gpt-4: The temperatures in Fahrenheit would be [716, 644, 77, 86, 752]\n"
     ]
    }
   ],
   "source": [
    "# Get the CoT answer and the PAL answer; check if they match\n",
    "pal_answer, _ = pal_results\n",
    "cot_answer, _ = cot_results\n",
    "\n",
    "# Verification threshold\n",
    "verification_threshold = 0.65\n",
    "\n",
    "# Extract the last number in the cot text\n",
    "last_digit = re.findall(r'(\\d+)(?![\\d\\S])', cot_answer)\n",
    "cot_answer = last_digit[-1] if last_digit else cot_answer\n",
    "# get the embeddings for the pal and cot_answers\n",
    "cot_embedding = embedding_model.encode(cot_answer, convert_to_tensor=True)\n",
    "pal_embedding = embedding_model.encode(pal_answer, convert_to_tensor=True)\n",
    "\n",
    "# compare the similarity between the embeddings\n",
    "similarity_score = util.pytorch_cos_sim(cot_embedding, pal_embedding).numpy()\n",
    "\n",
    "# accept answer if similarity score is greater than a threshold\n",
    "if similarity_score >= verification_threshold:\n",
    "    print(f\"Answer from verification using {weak_llm} : {cot_answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee651797-537f-4a0d-a7c2-a5702690fead",
   "metadata": {},
   "source": [
    "### Let's try the Tree of Thought for a more complex and subjective question using the vote based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6576cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: 1. Collaborating with other countries and private companies\n",
      "2. Utilizing advanced robotics and AI\n",
      "3. Sending unmanned missions to Mars\n",
      "\n",
      "Justification:\n",
      "- Collaborating with other countries and private companies offers the most promise as it allows for shared resources, expertise, and costs, leading to a more sustainable presence on Mars.\n",
      "- Utilizing advanced robotics and AI ranks second as it can enhance efficiency and productivity in tasks on Mars, but may face challenges with programming errors and unexpected advancements.\n",
      "- Sending unmanned missions to Mars ranks last as it may not be as cost-effective or sustainable in the long run compared to collaborative efforts.\n",
      "\n",
      "Steps to achieve the goal:\n",
      "1. Establish partnerships with other countries and private companies to share resources and expertise for Mars missions.\n",
      "2. Invest in research and development of advanced robotics and AI for tasks on Mars.\n",
      "3. Plan and launch joint missions and projects with clear objectives and responsibilities for a sustainable presence on Mars.\n",
      "\n",
      " gpt3.5 token usage and cost \n",
      "\n",
      "Tokens Used: 5617\n",
      "\tPrompt Tokens: 2817\n",
      "\tCompletion Tokens: 2800\n",
      "Successful Requests: 12\n",
      "Total Cost (USD): $0.009825500000000001\n",
      "\n",
      "\n",
      " gpt-4 token usage and cost \n",
      "\n",
      "\n",
      "Tokens Used: 353\n",
      "\tPrompt Tokens: 24\n",
      "\tCompletion Tokens: 329\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.02046\n"
     ]
    }
   ],
   "source": [
    "# Lets try ToT on a planning type question using the voting mechanism and compare the output with gpt-4\n",
    "# Running ToT takes time\n",
    "\n",
    "question = \"How to colonize Mars?\"\n",
    "\n",
    "with get_openai_callback() as tot_cb:\n",
    "    tot_answer, tot_s_score = asyncio.run(decision_by_vote(question=question, mode='tot'))\n",
    "    print(\"\\n gpt3.5 token usage and cost \\n\")\n",
    "    print(tot_cb)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Do not hallucinate an answer\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "gpt4_cot_llm_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-4\"), prompt=prompt)\n",
    "with get_openai_callback() as gpt4_tot_cb:\n",
    "    gpt4_answer = gpt4_cot_llm_chain.invoke(question)\n",
    "    print(\"\\n\\n gpt-4 token usage and cost \\n\\n\")\n",
    "    print(gpt4_tot_cb)\n",
    "\n",
    "tot_embedding = embedding_model.encode(tot_answer, convert_to_tensor=True)\n",
    "gpt4_embedding = embedding_model.encode(gpt4_answer['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between gpt-4 answer and gpt3.5 tot answer : 0.6508082747459412\n",
      "ToT 's' score : 0.8390442728996277\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the answers from the vote based ToT and GPT-4 match\n",
    "print(f\"Similarity between gpt-4 answer and gpt3.5 tot answer : {util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0]}\")\n",
    "print(f\"ToT 's' score : {tot_s_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d520aa0-6e7e-42ec-8cd2-77bc7782d592",
   "metadata": {},
   "source": [
    "### Let's compare the cost and the results for the ToT vote based method and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbbb93ff-943b-4a95-ba75-2c6e2ffdd7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_78dc3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_78dc3_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_78dc3_level0_col1\" class=\"col_heading level0 col1\" >ToT(# samples = 3)</th>\n",
       "      <th id=\"T_78dc3_level0_col2\" class=\"col_heading level0 col2\" >GPT-4 (# samples = 1)</th>\n",
       "      <th id=\"T_78dc3_level0_col3\" class=\"col_heading level0 col3\" >Answer similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_78dc3_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_78dc3_row0_col1\" class=\"data row0 col1\" >0.009830</td>\n",
       "      <td id=\"T_78dc3_row0_col2\" class=\"data row0 col2\" >0.020460</td>\n",
       "      <td id=\"T_78dc3_row0_col3\" class=\"data row0 col3\" > </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_78dc3_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_78dc3_row1_col1\" class=\"data row1 col1\" >1. Collaborating with other countries and private companies\n",
       "2. Utilizing advanced robotics and AI\n",
       "3. Sending unmanned missions to Mars\n",
       "\n",
       "Justification:\n",
       "- Collaborating with other countries and private companies offers the most promise as it allows for shared resources, expertise, and costs, leading to a more sustainable presence on Mars.\n",
       "- Utilizing advanced robotics and AI ranks second as it can enhance efficiency and productivity in tasks on Mars, but may face challenges with programming errors and unexpected advancements.\n",
       "- Sending unmanned missions to Mars ranks last as it may not be as cost-effective or sustainable in the long run compared to collaborative efforts.\n",
       "\n",
       "Steps to achieve the goal:\n",
       "1. Establish partnerships with other countries and private companies to share resources and expertise for Mars missions.\n",
       "2. Invest in research and development of advanced robotics and AI for tasks on Mars.\n",
       "3. Plan and launch joint missions and projects with clear objectives and responsibilities for a sustainable presence on Mars.</td>\n",
       "      <td id=\"T_78dc3_row1_col2\" class=\"data row1 col2\" >Colonizing Mars is a complex process that involves several stages. Here is a simplified breakdown:\n",
       "\n",
       "1. **Robotic Missions**: NASA and other space agencies have already started this process, sending unmanned missions to study Mars' climate, geology, and whether it ever had conditions suitable for life. \n",
       "\n",
       "2. **Sending Supplies Ahead of Time**: Before humans can live on Mars, they'll need stuff like food, water, shelter, and tools. These supplies will need to be sent ahead of time and landed safely on the Mars surface.\n",
       "\n",
       "3. **Human Mission to Mars**: The next step is to send humans, which is a much more difficult task due to the long journey, radiation exposure, and mental health concerns of the astronauts.\n",
       "\n",
       "4. **Building Infrastructure**: Once on Mars, astronauts will need to build the necessary infrastructure for sustainable living. This includes habitats, greenhouses for growing food, and systems for generating power and recycling water and air.\n",
       "\n",
       "5. **Establishing a Permanent Presence**: Over time, more supplies and people can be sent to Mars to establish a permanent presence. This will require regular supply missions and potentially the ability to use resources on Mars for life support and fuel.\n",
       "\n",
       "6. **Terraforming**: The long-term goal for some scientists and engineers is to terraform Mars, or make it more Earth-like by adding an atmosphere, warming the planet, and adding water.\n",
       "\n",
       "This process will take decades, if not centuries, and will require international cooperation, technological advancements, and significant resources. The feasibility, ethics, and potential benefits of colonizing Mars are subjects of ongoing debate among scientists, politicians, and the public.</td>\n",
       "      <td id=\"T_78dc3_row1_col3\" class=\"data row1 col3\" >0.650808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4d23540d00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'ToT(# samples = 3)': [round(tot_cb.total_cost, 5), tot_answer],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_tot_cb.total_cost, 5), gpt4_answer['text']],\n",
    "    'Answer similarity':[\" \",util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0] ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4ad27-b119-4e35-a72b-80abc45c60d3",
   "metadata": {},
   "source": [
    "### Lets compare the costs and answers for the different thought representations and GPT-4. \n",
    "#### In most cases the cascade/chain to follow would be CoT -> PAL -> MoT -> GPT4. Once PAL is run, MoT can be modified to reuse the PAL and CoT samples can be reused and therefore not incur any additional LLM costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d313eef1-60ac-49e3-a435-39e6701928e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
      "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c98c9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c98c9_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_c98c9_level0_col1\" class=\"col_heading level0 col1\" >CoT(# samples = 3)</th>\n",
       "      <th id=\"T_c98c9_level0_col2\" class=\"col_heading level0 col2\" >PAL (# samples = 3)</th>\n",
       "      <th id=\"T_c98c9_level0_col3\" class=\"col_heading level0 col3\" >MoT (# samples = 6)</th>\n",
       "      <th id=\"T_c98c9_level0_col4\" class=\"col_heading level0 col4\" >GPT-4 (# samples = 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c98c9_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_c98c9_row0_col1\" class=\"data row0 col1\" >0.001660</td>\n",
       "      <td id=\"T_c98c9_row0_col2\" class=\"data row0 col2\" >0.005880</td>\n",
       "      <td id=\"T_c98c9_row0_col3\" class=\"data row0 col3\" >0.006750</td>\n",
       "      <td id=\"T_c98c9_row0_col4\" class=\"data row0 col4\" >0.012870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c98c9_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_c98c9_row1_col1\" class=\"data row1 col1\" >32</td>\n",
       "      <td id=\"T_c98c9_row1_col2\" class=\"data row1 col2\" >[71.6, 64.4, 77.0, 86.0, 75.2]</td>\n",
       "      <td id=\"T_c98c9_row1_col3\" class=\"data row1 col3\" >[71.6, 64.4, 77.0, 86.0, 75.2]</td>\n",
       "      <td id=\"T_c98c9_row1_col4\" class=\"data row1 col4\" >To convert the temperatures from Celsius to Fahrenheit, we use the formula F = C * 9/5 + 32. \n",
       "\n",
       "For each temperature in the list:\n",
       "\n",
       "22°C = 22 * 9/5 + 32 = 71.6°F\n",
       "18°C = 18 * 9/5 + 32 = 64.4°F\n",
       "25°C = 25 * 9/5 + 32 = 77°F\n",
       "30°C = 30 * 9/5 + 32 = 86°F\n",
       "24°C = 24 * 9/5 + 32 = 75.2°F\n",
       "\n",
       "So, the converted list of temperatures in Fahrenheit is [71.6, 64.4, 77, 86, 75.2].</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4d044d2d00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'CoT(# samples = 3)': [round(cot_cb.total_cost, 5), cot_answer],\n",
    "    'PAL (# samples = 3)': [round(pal_cb.total_cost, 5), pal_answer],\n",
    "    'MoT (# samples = 6)': [round(mot_cb.total_cost, 5), mot_results[0]],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_cb.total_cost, 5), result['text']],\n",
    "}\n",
    "\n",
    "question  = \"\"\"\n",
    "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
    "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \"\"\"\n",
    "\n",
    "print(question)\n",
    "df = pd.DataFrame(data)\n",
    "df.style.hide(axis='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
