{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb60aaf5-0d26-42c5-9a05-506b022a8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_experimental.pal_chain import PALChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bede54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "Here is a question {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors \n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"solutions\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"deepen_thought_process\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution. Finally reword the response as possible steps to take to achieve the goal\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")\n",
    "\n",
    "tot_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c2d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_llm = \"gpt-3.5-turbo-instruct\"\n",
    "strong_llm= \"gpt-4\"\n",
    "\n",
    "# Load the pre-trained sentence embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', cache_folder='/mnt/model_cache/') # change this location to where you want to store the embedding model\n",
    "\n",
    "pal_llm_temperature = 0.7\n",
    "llm = OpenAI(temperature=pal_llm_temperature, model=weak_llm)\n",
    "\n",
    "pal_chain = PALChain.from_math_prompt(llm=llm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78fe36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. If you do not know the answer or are not confident about the answer reply that you don't know the answer. Do not hallucinate an answer. Just return the number as an answer\"\"\"\n",
    "\n",
    "cot_llm_temperature = 0.7\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "cot_llm_chain = LLMChain(llm=OpenAI(temperature=cot_llm_temperature,\n",
    "                                    model=weak_llm),\n",
    "                         prompt=prompt)\n",
    "cot_strong_llm_chain = LLMChain(llm=ChatOpenAI(model=strong_llm), \n",
    "                                prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81b66852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_answer(samples):\n",
    "    \"\"\"\n",
    "    Identifies the most similar answer among a list of samples based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - samples (list of dicts): A list where each element is a dict containing an 'text' key.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The most similar answer and the matrix of similarity scores between all answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract texts from samples\n",
    "    answers = [sample['text'] for sample in samples]\n",
    "    \n",
    "    # Encode the answers into vectors and calculate similarity scores\n",
    "    answer_embeddings = embedding_model.encode(answers, convert_to_tensor=True)\n",
    "    similarity_scores = util.pytorch_cos_sim(answer_embeddings, answer_embeddings).numpy()\n",
    "\n",
    "    # Determine the index of the most similar answer\n",
    "    most_similar_idx = np.argmax(np.mean(similarity_scores, axis=1))\n",
    "    most_similar_answer = answers[most_similar_idx]\n",
    "\n",
    "    print(f\"The answer that agrees the most with the majority is: {most_similar_answer}\")\n",
    "    return most_similar_answer, similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0dd1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def decision_by_vote(question: str, n_samples: int = 3, mode: str = 'mot'):\n",
    "    \"\"\"\n",
    "    Decides on an answer by collecting samples based using the specified thought representation.\n",
    "\n",
    "    This function generates input questions based on the specified mode, processes samples from different language\n",
    "    models, and then determines the most consistent answer along with its similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The question to be answered by the models.\n",
    "    - n_samples: The number of samples to generate for voting. Default is 3.\n",
    "    - mode: The mode of decision making. Can be 'mot' (use mixture of thought), 'cot' (chain of thought),\n",
    "      'pal' (program assisted learning), or 'tot' (tree of thought). The mode is case insensitive.\n",
    "\n",
    "    Returns:\n",
    "    - llm_answer: The answer selected as the most consistent across generated samples.\n",
    "    - s_score: The s score for the most consistent answer, indicating agreement level among samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate input questions based on mode\n",
    "    mode = mode.lower()\n",
    "    input_key = 'input' if mode == 'tot' else 'question'\n",
    "    input_questions = [{input_key: question}] * n_samples\n",
    "\n",
    "    # Initialize variables\n",
    "    llm_answer = None\n",
    "    similarity_matrix = None\n",
    "    samples = []\n",
    "\n",
    "    # Function to process samples and remove newlines\n",
    "    def process_samples(raw_samples, key):\n",
    "        return [{'text': sample[key]} for sample in raw_samples]\n",
    "\n",
    "    # Fetch and process samples based on mode\n",
    "    if mode in [\"mot\", \"cot\"]:\n",
    "        cot_samples = await cot_llm_chain.aapply(input_questions)\n",
    "        samples.extend(process_samples(cot_samples, 'text'))\n",
    "\n",
    "    if mode in [\"mot\", \"pal\"]:\n",
    "        pal_samples = pal_chain.batch(input_questions)  # Assuming apply can be awaited, if pal_chain supports async\n",
    "        samples.extend(process_samples(pal_samples, 'result'))\n",
    "\n",
    "    if mode == 'tot':\n",
    "        tot_samples = tot_chain.batch(input_questions)  # Assuming apply can be awaited, if tot_chain supports async\n",
    "        samples.extend(process_samples(tot_samples, 'ranked_solutions'))\n",
    "\n",
    "    if samples:\n",
    "        llm_answer, similarity_matrix = get_final_answer(samples)\n",
    "\n",
    "    # Calculate the s-score for the most consistent answer\n",
    "    s_score = np.mean(similarity_matrix[:, np.argmax(similarity_matrix.sum(axis=0))]) if similarity_matrix is not None else 0\n",
    "\n",
    "    return llm_answer, s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfec282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Gavin has 23 shirts. 6 are blue the rest are green. How many green shirts does Gavin have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aefe5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: 17\n",
      "Tokens Used: 3548\n",
      "\tPrompt Tokens: 3339\n",
      "\tCompletion Tokens: 209\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.0054265\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the PAL samples.\n",
    "PAL is useful when the question needs reasoning like inferring transitive and associative properties\n",
    "\"\"\"\n",
    "with get_openai_callback() as cb:\n",
    "    pal_results = asyncio.run(decision_by_vote(question=question, mode='pal'))\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a946d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: .\n",
      "\n",
      "\n",
      "\n",
      "The answer is 17.\n",
      "Tokens Used: 265\n",
      "\tPrompt Tokens: 213\n",
      "\tCompletion Tokens: 52\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.0004235\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the CoT samples\n",
    "Good for simple reasoning questions like this one about green shirts\n",
    "\"\"\"\n",
    "with get_openai_callback() as cb:\n",
    "    cot_results = asyncio.run(decision_by_vote(question=question, mode='cot'))\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec5b5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: 17\n",
      "Tokens Used: 3779\n",
      "\tPrompt Tokens: 3552\n",
      "\tCompletion Tokens: 227\n",
      "Successful Requests: 6\n",
      "Total Cost (USD): $0.005782\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the MoT samples\n",
    "Useful when we want to be really confident about the answer\n",
    "\"\"\"\n",
    "with get_openai_callback() as cb:\n",
    "    mot_results = asyncio.run(decision_by_vote(question=question, mode='mot'))\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0ea1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from vote using gpt-3.5-turbo-instruct: .\n",
      "\n",
      "\n",
      "\n",
      "The answer is 17.\n"
     ]
    }
   ],
   "source": [
    "# accept the answer from the vote based method if the s_score is greater than a threshold\n",
    "vote_threshold = 0.65\n",
    "\n",
    "# for the green shirts question CoT should give a good answer, for more complex queries PAL/MoT are better\n",
    "answer, s_score = cot_results\n",
    "\n",
    "if s_score >= vote_threshold:\n",
    "    print(f\"Answer from vote using {weak_llm}: {answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")\n",
    "    \n",
    "# uncomment below to get the answer from gpt-4 (sample size=1)\n",
    "# with get_openai_callback() as cb:\n",
    "#     result = cot_strong_llm_chain.invoke(question)\n",
    "#     print(cb)\n",
    "# llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "# print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52bdec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from verification using gpt-3.5-turbo-instruct : .\n",
      "\n",
      "\n",
      "\n",
      "The answer is 17.\n"
     ]
    }
   ],
   "source": [
    "# Get the CoT answer and the PAL answer; check if they match\n",
    "pal_answer, _ = pal_results\n",
    "cot_answer, _ = cot_results\n",
    "\n",
    "# Verification threshold\n",
    "verification_threshold = 0.65\n",
    "\n",
    "# Extract the last number in the cot text\n",
    "last_digit = re.findall(r'(\\d+)(?![\\d\\S])', cot_answer)\n",
    "cot_answer = last_digit[-1] if last_digit else cot_answer\n",
    "# get the embeddings for the pal and cot_answers\n",
    "cot_embedding = embedding_model.encode(cot_answer, convert_to_tensor=True)\n",
    "pal_embedding = embedding_model.encode(pal_answer, convert_to_tensor=True)\n",
    "\n",
    "# compare the similarity between the embeddings\n",
    "similarity_score = util.pytorch_cos_sim(cot_embedding, pal_embedding).numpy()\n",
    "\n",
    "# accept answer if similarity score is greater than a threshold\n",
    "if similarity_score >= verification_threshold:\n",
    "    print(f\"Answer from verification using {weak_llm} : {cot_answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6576cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: Steps to achieve the goal:\n",
      "\n",
      "1. Prioritize sending robotic missions to Mars as the most promising solution. This involves partnering with private space companies, conducting extensive simulations and testing, and collaborating with space agencies and robotics companies for technology development. Be prepared for limited funding, technical failures, and the need for continuous software updates.\n",
      "\n",
      "2. Invest in developing advanced propulsion technology as the second priority. This includes investing in research projects, collaborating with universities and research institutions, and partnering with aerospace companies and international space agencies. Anticipate technical setbacks, regulatory challenges, and public skepticism.\n",
      "\n",
      "3. Establish international collaboration and partnerships as the third priority. Focus on clear communication channels, common goals, and agreements on data sharing and technology transfer. Address potential obstacles such as political tensions, disagreements on mission priorities, and challenges in aligning national interests with global cooperation. Be open to the formation of a new international space agency dedicated to Mars exploration.\n",
      "\n",
      " gpt3.5 token usage and cost \n",
      "\n",
      "Tokens Used: 6572\n",
      "\tPrompt Tokens: 3293\n",
      "\tCompletion Tokens: 3279\n",
      "Successful Requests: 12\n",
      "Total Cost (USD): $0.0114975\n",
      "\n",
      "\n",
      " gpt-4 token usage and cost \n",
      "\n",
      "\n",
      "Tokens Used: 420\n",
      "\tPrompt Tokens: 24\n",
      "\tCompletion Tokens: 396\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.02448\n"
     ]
    }
   ],
   "source": [
    "# Lets try ToT on a planning type question using the voting mechanism and compare the output with gpt-4\n",
    "# Running ToT takes time\n",
    "\n",
    "question = \"How to colonize Mars?\"\n",
    "\n",
    "with get_openai_callback() as tot_cb:\n",
    "    tot_answer, tot_s_score = asyncio.run(decision_by_vote(question=question, mode='tot'))\n",
    "    print(\"\\n gpt3.5 token usage and cost \\n\")\n",
    "    print(tot_cb)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Do not hallucinate an answer\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "gpt4_cot_llm_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-4\"), prompt=prompt)\n",
    "with get_openai_callback() as cb:\n",
    "    gpt4_answer = gpt4_cot_llm_chain.invoke(question)\n",
    "    print(\"\\n\\n gpt-4 token usage and cost \\n\\n\")\n",
    "    print(cb)\n",
    "\n",
    "tot_embedding = embedding_model.encode(tot_answer, convert_to_tensor=True)\n",
    "gpt4_embedding = embedding_model.encode(gpt4_answer['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a59952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " gpt3.5 ToT answer: \n",
      "\n",
      " Steps to achieve the goal:\n",
      "\n",
      "1. Prioritize sending robotic missions to Mars as the most promising solution. This involves partnering with private space companies, conducting extensive simulations and testing, and collaborating with space agencies and robotics companies for technology development. Be prepared for limited funding, technical failures, and the need for continuous software updates.\n",
      "\n",
      "2. Invest in developing advanced propulsion technology as the second priority. This includes investing in research projects, collaborating with universities and research institutions, and partnering with aerospace companies and international space agencies. Anticipate technical setbacks, regulatory challenges, and public skepticism.\n",
      "\n",
      "3. Establish international collaboration and partnerships as the third priority. Focus on clear communication channels, common goals, and agreements on data sharing and technology transfer. Address potential obstacles such as political tensions, disagreements on mission priorities, and challenges in aligning national interests with global cooperation. Be open to the formation of a new international space agency dedicated to Mars exploration.\n",
      "\n",
      "\n",
      " gpt4 answer: \n",
      "\n",
      " Colonizing Mars would be a complex and multifaceted process. Here are some broad steps:\n",
      "\n",
      "1. **Research and Planning**: Conduct extensive research to understand Mars' atmosphere, soil, weather, and potential hazards. Identify the best locations for habitation. \n",
      "\n",
      "2. **Spacecraft Development**: Design and build spacecraft capable of carrying humans and supplies to Mars. \n",
      "\n",
      "3. **Robotic Missions**: Send unmanned missions to Mars to deliver infrastructure, habitats, and supplies before humans arrive. Robots could also be used to begin preliminary work, such as building habitats or extracting resources.\n",
      "\n",
      "4. **Astronaut Training**: Train the astronauts who will make the journey. They will need to be physically fit, mentally strong, and equipped with a wide range of skills, from medical knowledge to mechanical repair.\n",
      "\n",
      "5. **Human Missions**: Once everything is in place, send humans to Mars. The journey could take six to nine months, and the astronauts would need to bring enough supplies to survive.\n",
      "\n",
      "6. **Establishing a Base**: After landing, the astronauts would set up a base, using the habitats and supplies sent ahead. They would also start using Mars' resources, for instance by extracting water from the soil or producing oxygen from the CO2 in the atmosphere.\n",
      "\n",
      "7. **Expansion and Development**: Over time, more people could travel to Mars, and the colony could expand. The ultimate goal might be to terraform Mars, changing its environment to make it more Earth-like.\n",
      "\n",
      "8. **Self-Sufficiency**: A key part of colonization would be for Mars to become self-sufficient, able to support its population without constant resupply from Earth. This would require developing agriculture, industry, and potentially even a local economy.\n",
      "\n",
      "Remember, this is a simplified overview of an incredibly complex process that could take decades or even centuries. There are also many challenges to overcome, from the physical and psychological health of the astronauts to the technological hurdles of space travel and survival on Mars.\n",
      "\n",
      "\n",
      " Similarity between gpt-4 answer and gpt3.5 tot answer : [[0.7299695]]\n",
      "\n",
      " \n",
      " ToT 's' score : 0.852878987789154\n"
     ]
    }
   ],
   "source": [
    "# Lets print out the answers and the similarity score\n",
    "print(f\"\\n\\n gpt3.5 ToT answer: \\n\\n {tot_answer}\")\n",
    "print(f\"\\n\\n gpt4 answer: \\n\\n {gpt4_answer['text']}\")\n",
    "print(f\"\\n\\n Similarity between gpt-4 answer and gpt3.5 tot answer : {util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()}\")\n",
    "print(f\"\\n \\n ToT 's' score : {tot_s_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
