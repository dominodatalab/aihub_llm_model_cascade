{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c730f9b8-3dbe-4ba9-9329-b583e8756c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_experimental.pal_chain import PALChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Note : This program has OPENAI_API_KEY set as an environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bc56e-846f-4c0e-9124-2f735fc41d9f",
   "metadata": {},
   "source": [
    "### Let's set up the template for Tree of Thought. We'll use this for tasks that involve planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bede54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "Here is a question {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors \n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"solutions\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"deepen_thought_process\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution. Finally reword the response as possible steps to take to achieve the goal\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")\n",
    "\n",
    "tot_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d819b-b31e-4e93-ae51-ef7d4331af68",
   "metadata": {},
   "source": [
    "### Set up the LLMs, embedding model and PAL chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c2d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_llm = \"gpt-3.5-turbo-instruct\"\n",
    "strong_llm= \"gpt-4\"\n",
    "\n",
    "# Load the pre-trained sentence embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', cache_folder='/mnt/artifacts/model_cache/') # change this location to where you want to store the embedding model\n",
    "\n",
    "pal_llm_temperature = 0.7\n",
    "llm = OpenAI(temperature=pal_llm_temperature, model=weak_llm)\n",
    "\n",
    "pal_chain = PALChain.from_math_prompt(llm=llm, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5eca2e-9394-41ee-97f9-97444d36bfc2",
   "metadata": {},
   "source": [
    "### Setup the CoT chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "78fe36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. If you do not know the answer or are not confident about the answer reply that you don't know the answer. Do not hallucinate an answer. Just return the number as an answer\"\"\"\n",
    "\n",
    "cot_llm_temperature = 0.7\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "cot_llm_chain = LLMChain(llm=OpenAI(temperature=cot_llm_temperature,\n",
    "                                    model=weak_llm),\n",
    "                         prompt=prompt)\n",
    "cot_strong_llm_chain = LLMChain(llm=ChatOpenAI(model=strong_llm), \n",
    "                                prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6decaa9e-521e-4713-99e8-302bd6ab3443",
   "metadata": {},
   "source": [
    "### Helper function to return the answer that agrees most with the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "81b66852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_answer(samples):\n",
    "    \"\"\"\n",
    "    Identifies the most similar answer among a list of samples based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - samples (list of dicts): A list where each element is a dict containing an 'text' key.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The most similar answer and the matrix of similarity scores between all answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract texts from samples\n",
    "    answers = [sample['text'] for sample in samples]\n",
    "    \n",
    "    # Encode the answers into vectors and calculate similarity scores\n",
    "    answer_embeddings = embedding_model.encode(answers, convert_to_tensor=True)\n",
    "    similarity_scores = util.pytorch_cos_sim(answer_embeddings, answer_embeddings).numpy()\n",
    "\n",
    "    # Determine the index of the most similar answer\n",
    "    most_similar_idx = np.argmax(np.mean(similarity_scores, axis=1))\n",
    "    most_similar_answer = answers[most_similar_idx]\n",
    "\n",
    "    print(f\"The answer that agrees the most with the majority is: {most_similar_answer}\")\n",
    "    return most_similar_answer, similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8ddd5-a5c3-47dd-8263-cbb71635d2ef",
   "metadata": {},
   "source": [
    "### Implementation of the decision by vote where multiple answers are sampled and the answer that agrees the most with the sampled answer is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0dd1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def decision_by_vote(question: str, n_samples: int = 3, mode: str = 'mot'):\n",
    "    \"\"\"\n",
    "    Decides on an answer by collecting samples based using the specified thought representation.\n",
    "\n",
    "    This function generates sample answers based on the specified mode and then determines the most consistent answer along with its similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The question to be answered by the models.\n",
    "    - n_samples: The number of samples to generate for voting. Default is 3.\n",
    "    - mode: The mode of decision making. Can be 'mot' (use mixture of thought), 'cot' (chain of thought),\n",
    "      'pal' (program assisted learning), or 'tot' (tree of thought). The mode is case insensitive.\n",
    "\n",
    "    Returns:\n",
    "    - llm_answer: The answer selected as the most consistent across generated samples.\n",
    "    - s_score: The s score for the most consistent answer, indicating agreement level among samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate input questions based on mode\n",
    "    mode = mode.lower()\n",
    "    input_key = 'input' if mode == 'tot' else 'question'\n",
    "    input_questions = [{input_key: question}] * n_samples\n",
    "\n",
    "    # Initialize variables\n",
    "    llm_answer = None\n",
    "    similarity_matrix = None\n",
    "    samples = []\n",
    "\n",
    "    # Function to process samples and remove newlines\n",
    "    def process_samples(raw_samples, key):\n",
    "        return [{'text': sample[key]} for sample in raw_samples]\n",
    "\n",
    "    # Fetch and process samples based on mode\n",
    "    if mode in [\"mot\", \"cot\"]:\n",
    "        cot_samples = await cot_llm_chain.aapply(input_questions)\n",
    "        samples.extend(process_samples(cot_samples, 'text'))\n",
    "\n",
    "    if mode in [\"mot\", \"pal\"]:\n",
    "        pal_samples = pal_chain.batch(input_questions)  # Assuming apply can be awaited, if pal_chain supports async\n",
    "        samples.extend(process_samples(pal_samples, 'result'))\n",
    "\n",
    "    if mode == 'tot':\n",
    "        tot_samples = tot_chain.batch(input_questions)  # Assuming apply can be awaited, if tot_chain supports async\n",
    "        samples.extend(process_samples(tot_samples, 'ranked_solutions'))\n",
    "\n",
    "    if samples:\n",
    "        llm_answer, similarity_matrix = get_final_answer(samples)\n",
    "\n",
    "    # Calculate the s-score for the most consistent answer\n",
    "    s_score = np.mean(similarity_matrix[:, np.argmax(similarity_matrix.sum(axis=0))]) if similarity_matrix is not None else 0\n",
    "\n",
    "    return llm_answer, s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dfec282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question to be used for the different thought representations\n",
    "\n",
    "question  = \"\"\"\n",
    "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
    "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43d638-baca-4b4e-a72e-14c41cee367a",
   "metadata": {},
   "source": [
    "# Get the answers from the vote based method for the different thought representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aefe5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 3789\n",
      "\tPrompt Tokens: 3432\n",
      "\tCompletion Tokens: 357\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.005861999999999999\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the PAL samples.\n",
    "PAL is useful when the question needs reasoning like inferring transitive and associative properties\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "with get_openai_callback() as pal_cb:\n",
    "    pal_results = asyncio.run(decision_by_vote(question=question, mode='pal'))\n",
    "    print(pal_cb)\n",
    "# pal_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8a946d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: .\n",
      "\n",
      "The list of temperatures in Fahrenheit would be [71.6, 64.4, 77, 86, 75.2].\n",
      "Tokens Used: 492\n",
      "\tPrompt Tokens: 306\n",
      "\tCompletion Tokens: 186\n",
      "Successful Requests: 3\n",
      "Total Cost (USD): $0.0008309999999999999\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the CoT samples\n",
    "Good for simple reasoning questions like this one about green shirts\n",
    "\"\"\"\n",
    "with get_openai_callback() as cot_cb:\n",
    "    cot_results = asyncio.run(decision_by_vote(question=question, mode='cot'))\n",
    "    print(cot_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b642c-9c2c-4719-88b0-c6550c5c4f9a",
   "metadata": {},
   "source": [
    "#### Note : This will resample from CoT and PAL and will not reuse the samples from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec5b5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: [71.6, 64.4, 77.0, 86.0, 75.2]\n",
      "Tokens Used: 4605\n",
      "\tPrompt Tokens: 3738\n",
      "\tCompletion Tokens: 867\n",
      "Successful Requests: 6\n",
      "Total Cost (USD): $0.007341\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the most consistent answer from the MoT samples\n",
    "Useful when we want to be really confident about the answer\n",
    "\"\"\"\n",
    "with get_openai_callback() as mot_cb:\n",
    "    mot_results = asyncio.run(decision_by_vote(question=question, mode='mot'))\n",
    "    print(mot_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bab2d2-fd2f-4bf8-b463-8bdd9f460d01",
   "metadata": {},
   "source": [
    "### Decide whether to accept/reject and call the stronger LLM to get an answer for the vote based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0ea1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from vote using gpt-3.5-turbo-instruct: .\n",
      "\n",
      "The list of temperatures in Fahrenheit would be [71.6, 64.4, 77, 86, 75.2].\n"
     ]
    }
   ],
   "source": [
    "# accept the answer from the vote based method if the s_score is greater than a threshold\n",
    "vote_threshold = 0.65\n",
    "\n",
    "# for the green shirts question CoT should give a good answer, for more complex queries PAL/MoT are better\n",
    "answer, s_score = cot_results\n",
    "\n",
    "if s_score >= vote_threshold:\n",
    "    print(f\"Answer from vote using {weak_llm}: {answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")\n",
    "    \n",
    "# uncomment below to get the answer from gpt-4 (sample size=1)\n",
    "# with get_openai_callback() as gpt4_cb:\n",
    "#     result = cot_strong_llm_chain.invoke(question)\n",
    "#     print(gpt4_cb)\n",
    "# llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "# print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d300b-850b-4ce1-b93f-20b5c2ed28e2",
   "metadata": {},
   "source": [
    "### Decide whether to accept/reject and call the stronger LLM to get an answer for the decision based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52bdec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 301\n",
      "\tPrompt Tokens: 109\n",
      "\tCompletion Tokens: 192\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.01479\n",
      "Answer from gpt-4: To convert each temperature from Celsius to Fahrenheit, we use the formula F = C * 9/5 + 32 Let's apply this to each temperature:- For 22 degrees Celsius: F = 22 * 9/5 + 32 = 716 degrees Fahrenheit- For 18 degrees Celsius: F = 18 * 9/5 + 32 = 644 degrees Fahrenheit- For 25 degrees Celsius: F = 25 * 9/5 + 32 = 77 degrees Fahrenheit- For 30 degrees Celsius: F = 30 * 9/5 + 32 = 86 degrees Fahrenheit- For 24 degrees Celsius: F = 24 * 9/5 + 32 = 752 degrees FahrenheitSo the converted temperatures in Fahrenheit are [716, 644, 77, 86, 752]\n"
     ]
    }
   ],
   "source": [
    "# Get the CoT answer and the PAL answer; check if they match\n",
    "pal_answer, _ = pal_results\n",
    "cot_answer, _ = cot_results\n",
    "\n",
    "# Verification threshold\n",
    "verification_threshold = 0.65\n",
    "\n",
    "# Extract the last number in the cot text\n",
    "last_digit = re.findall(r'(\\d+)(?![\\d\\S])', cot_answer)\n",
    "cot_answer = last_digit[-1] if last_digit else cot_answer\n",
    "# get the embeddings for the pal and cot_answers\n",
    "cot_embedding = embedding_model.encode(cot_answer, convert_to_tensor=True)\n",
    "pal_embedding = embedding_model.encode(pal_answer, convert_to_tensor=True)\n",
    "\n",
    "# compare the similarity between the embeddings\n",
    "similarity_score = util.pytorch_cos_sim(cot_embedding, pal_embedding).numpy()\n",
    "\n",
    "# accept answer if similarity score is greater than a threshold\n",
    "if similarity_score >= verification_threshold:\n",
    "    print(f\"Answer from verification using {weak_llm} : {cot_answer}\")\n",
    "    \n",
    "else:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = cot_strong_llm_chain.invoke(question)\n",
    "        print(cb)\n",
    "    llm_answer = result['text'].replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "    print(f\"Answer from {strong_llm}: {llm_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee651797-537f-4a0d-a7c2-a5702690fead",
   "metadata": {},
   "source": [
    "### Let's try the Tree of Thought for a more complex and subjective question using the vote based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6576cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that agrees the most with the majority is: 1. Sending robotic missions first\n",
      "2. Building underground habitats\n",
      "3. Terraforming Mars\n",
      "\n",
      "Justification:\n",
      "Sending robotic missions first is ranked highest as it allows for gathering essential data and preparing for future human missions. Building underground habitats is ranked second as it provides a more feasible and immediate solution for sustaining life on Mars. Terraforming Mars is ranked last as it presents significant challenges and uncertainties that may not be achievable in the near future.\n",
      "\n",
      "Steps to achieve the goal:\n",
      "1. Develop autonomous robots for Mars exploration\n",
      "2. Conduct feasibility studies for underground habitats on Mars\n",
      "3. Collaborate with international partners for terraforming research and development.\n",
      "\n",
      " gpt3.5 token usage and cost \n",
      "\n",
      "Tokens Used: 7083\n",
      "\tPrompt Tokens: 3586\n",
      "\tCompletion Tokens: 3497\n",
      "Successful Requests: 12\n",
      "Total Cost (USD): $0.012373\n",
      "\n",
      "\n",
      " gpt-4 token usage and cost \n",
      "\n",
      "\n",
      "Tokens Used: 424\n",
      "\tPrompt Tokens: 24\n",
      "\tCompletion Tokens: 400\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.02472\n"
     ]
    }
   ],
   "source": [
    "# Lets try ToT on a planning type question using the voting mechanism and compare the output with gpt-4\n",
    "# Running ToT takes time\n",
    "\n",
    "question = \"How to colonize Mars?\"\n",
    "\n",
    "with get_openai_callback() as tot_cb:\n",
    "    tot_answer, tot_s_score = asyncio.run(decision_by_vote(question=question, mode='tot'))\n",
    "    print(\"\\n gpt3.5 token usage and cost \\n\")\n",
    "    print(tot_cb)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Do not hallucinate an answer\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "gpt4_cot_llm_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-4\"), prompt=prompt)\n",
    "with get_openai_callback() as gpt4_tot_cb:\n",
    "    gpt4_answer = gpt4_cot_llm_chain.invoke(question)\n",
    "    print(\"\\n\\n gpt-4 token usage and cost \\n\\n\")\n",
    "    print(gpt4_tot_cb)\n",
    "\n",
    "tot_embedding = embedding_model.encode(tot_answer, convert_to_tensor=True)\n",
    "gpt4_embedding = embedding_model.encode(gpt4_answer['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a59952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between gpt-4 answer and gpt3.5 tot answer : 0.6983373165130615\n",
      "ToT 's' score : 0.8805679678916931\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the answers from the vote based ToT and GPT-4 match\n",
    "print(f\"Similarity between gpt-4 answer and gpt3.5 tot answer : {util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0]}\")\n",
    "print(f\"ToT 's' score : {tot_s_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d520aa0-6e7e-42ec-8cd2-77bc7782d592",
   "metadata": {},
   "source": [
    "### Let's compare the cost and the results for the ToT vote based method and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bbbb93ff-943b-4a95-ba75-2c6e2ffdd7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bc408\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_bc408_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_bc408_level0_col1\" class=\"col_heading level0 col1\" >ToT(# samples = 3)</th>\n",
       "      <th id=\"T_bc408_level0_col2\" class=\"col_heading level0 col2\" >GPT-4 (# samples = 1)</th>\n",
       "      <th id=\"T_bc408_level0_col3\" class=\"col_heading level0 col3\" >Answer similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_bc408_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_bc408_row0_col1\" class=\"data row0 col1\" >0.012370</td>\n",
       "      <td id=\"T_bc408_row0_col2\" class=\"data row0 col2\" >0.024720</td>\n",
       "      <td id=\"T_bc408_row0_col3\" class=\"data row0 col3\" > </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc408_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_bc408_row1_col1\" class=\"data row1 col1\" >1. Sending robotic missions first\n",
       "2. Building underground habitats\n",
       "3. Terraforming Mars\n",
       "\n",
       "Justification:\n",
       "Sending robotic missions first is ranked highest as it allows for gathering essential data and preparing for future human missions. Building underground habitats is ranked second as it provides a more feasible and immediate solution for sustaining life on Mars. Terraforming Mars is ranked last as it presents significant challenges and uncertainties that may not be achievable in the near future.\n",
       "\n",
       "Steps to achieve the goal:\n",
       "1. Develop autonomous robots for Mars exploration\n",
       "2. Conduct feasibility studies for underground habitats on Mars\n",
       "3. Collaborate with international partners for terraforming research and development.</td>\n",
       "      <td id=\"T_bc408_row1_col2\" class=\"data row1 col2\" >Colonizing Mars is a complex process, but here are possible steps that could be involved:\n",
       "\n",
       "1. Investigate and Understand: First, we need to gain a thorough understanding of Mars. This involves researching its geology, climate, potential resources, and any potential hazards. This can be done through robotic missions, satellites, and telescopes.\n",
       "\n",
       "2. Develop Technology: We need to develop the technology necessary for travel to Mars, including advanced propulsion systems to shorten the journey, spacecraft capable of carrying humans and cargo, and systems to provide life support for the astronauts during the journey.\n",
       "\n",
       "3. Test Missions: Before sending humans, several unmanned missions should be conducted to test equipment, conduct more research, and potentially set up initial infrastructure.\n",
       "\n",
       "4. Human Missions: After all the tests and preparations are done, humans could be sent to Mars. The first few missions might be short-term, but they would pave the way for longer stays.\n",
       "\n",
       "5. Establish a Base: The next step would be to establish a permanent base. This would likely involve the use of habitats that protect against radiation, supply breathable air, and can maintain a comfortable temperature. \n",
       "\n",
       "6. Utilize Martian Resources: To sustain a colony, we would need to utilize resources found on Mars. This could involve mining for water ice, which could be used for drinking water, growing food, and even producing fuel.\n",
       "\n",
       "7. Expand and Develop: Once a base is established and resources are being utilized, the colony could begin to expand. This would involve building more habitats, developing agriculture, and even setting up industries.\n",
       "\n",
       "8. Terraforming (Optional): This is a hypothetical step that involves transforming Mars' environment to make it more Earth-like. It could involve warming the planet and thickening its atmosphere.\n",
       "\n",
       "Remember, these steps are just a broad outline. The actual process would be far more complex and would require the cooperation of many nations and potentially private companies. It would also take a long time - possibly centuries - and would require substantial resources.</td>\n",
       "      <td id=\"T_bc408_row1_col3\" class=\"data row1 col3\" >0.698337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdd7e464ca0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'ToT(# samples = 3)': [round(tot_cb.total_cost, 5), tot_answer],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_tot_cb.total_cost, 5), gpt4_answer['text']],\n",
    "    'Answer similarity':[\" \",util.pytorch_cos_sim(gpt4_embedding, tot_embedding).numpy()[0][0] ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4ad27-b119-4e35-a72b-80abc45c60d3",
   "metadata": {},
   "source": [
    "### Lets compare the costs and answers for the different thought representations and GPT-4. \n",
    "#### In most cases the cascade/chain to follow would be CoT -> PAL -> MoT -> GPT4. Once PAL is run, MoT will not incur any additional LLM costs as the PAL and CoT samples can be reused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d313eef1-60ac-49e3-a435-39e6701928e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
      "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ced58\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ced58_level0_col0\" class=\"col_heading level0 col0\" >Technique</th>\n",
       "      <th id=\"T_ced58_level0_col1\" class=\"col_heading level0 col1\" >CoT(# samples = 3)</th>\n",
       "      <th id=\"T_ced58_level0_col2\" class=\"col_heading level0 col2\" >PAL (# samples = 3)</th>\n",
       "      <th id=\"T_ced58_level0_col3\" class=\"col_heading level0 col3\" >MoT (# samples = 6)</th>\n",
       "      <th id=\"T_ced58_level0_col4\" class=\"col_heading level0 col4\" >GPT-4 (# samples = 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ced58_row0_col0\" class=\"data row0 col0\" >Cost($ USD)</td>\n",
       "      <td id=\"T_ced58_row0_col1\" class=\"data row0 col1\" >0.000830</td>\n",
       "      <td id=\"T_ced58_row0_col2\" class=\"data row0 col2\" >0.005860</td>\n",
       "      <td id=\"T_ced58_row0_col3\" class=\"data row0 col3\" >0.007340</td>\n",
       "      <td id=\"T_ced58_row0_col4\" class=\"data row0 col4\" >0.014190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ced58_row1_col0\" class=\"data row1 col0\" >Answer</td>\n",
       "      <td id=\"T_ced58_row1_col1\" class=\"data row1 col1\" >.\n",
       "\n",
       "The list of temperatures in Fahrenheit would be [71.6, 64.4, 77, 86, 75.2].</td>\n",
       "      <td id=\"T_ced58_row1_col2\" class=\"data row1 col2\" >[71.6, 64.4, 77.0, 86.0, 75.2]</td>\n",
       "      <td id=\"T_ced58_row1_col3\" class=\"data row1 col3\" >[71.6, 64.4, 77.0, 86.0, 75.2]</td>\n",
       "      <td id=\"T_ced58_row1_col4\" class=\"data row1 col4\" >To convert each temperature from Celsius to Fahrenheit, we use the formula F = C * 9/5 + 32. Let's apply this to each temperature:\n",
       "\n",
       "- For 22 degrees Celsius: F = 22 * 9/5 + 32 = 71.6 degrees Fahrenheit\n",
       "- For 18 degrees Celsius: F = 18 * 9/5 + 32 = 64.4 degrees Fahrenheit\n",
       "- For 25 degrees Celsius: F = 25 * 9/5 + 32 = 77 degrees Fahrenheit\n",
       "- For 30 degrees Celsius: F = 30 * 9/5 + 32 = 86 degrees Fahrenheit\n",
       "- For 24 degrees Celsius: F = 24 * 9/5 + 32 = 75.2 degrees Fahrenheit\n",
       "\n",
       "So the converted temperatures in Fahrenheit are [71.6, 64.4, 77, 86, 75.2].</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdd7f36e6d0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Technique': ['Cost($ USD)', 'Answer'],\n",
    "    'CoT(# samples = 3)': [round(cot_cb.total_cost, 5), cot_answer],\n",
    "    'PAL (# samples = 3)': [round(pal_cb.total_cost, 5), pal_answer],\n",
    "    'MoT (# samples = 6)': [round(mot_cb.total_cost, 5), mot_results[0]],\n",
    "    'GPT-4 (# samples = 1)': [round(gpt4_cb.total_cost, 5), result['text']],\n",
    "}\n",
    "\n",
    "question  = \"\"\"\n",
    "Imagine you have a list of temperatures in Celsius from various cities on a particular day: [22, 18, 25, 30, 24].\n",
    "convert these temperatures to Fahrenheit using the formula F = C * 9/5 + 32? \"\"\"\n",
    "\n",
    "print(question)\n",
    "df = pd.DataFrame(data)\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325e212-dd1e-486f-a68a-eefc9a00ba60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
